{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCNfGb8C539wKxQiWXKswy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidimprovz/experiments/blob/main/docsGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsRINeBdFQdG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/imartinez/privateGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remember you'll have to restart runtime after install\n",
        "!pip install -r ./privateGPT/requirements.txt"
      ],
      "metadata": {
        "id": "gTvsOYLKL2HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin -P models/"
      ],
      "metadata": {
        "id": "cqxsSatuL2y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remmeber to edit runtime file for different optimizations\n",
        "# !touch privateGPT/runtime.env"
      ],
      "metadata": {
        "id": "-JKLfKMFL9_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp privateGPT/runtime.env .env"
      ],
      "metadata": {
        "id": "rTyPzfBlHsxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('privateGPT/example.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA9jX6yFLguj",
        "outputId": "e33e787e-067e-4a5a-b76d-ed1cc75f202f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remember: either edit ingest.py file to specify source_docs, or move privaateGPT contents one level up.\n",
        "!python privateGPT/ingest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbCClXzXNmQu",
        "outputId": "7c327c05-a739-4259-d892-14a8cb6ac53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "db\n",
            "Creating new vectorstore\n",
            "Loading documents from privateGPT/source_documents\n",
            "Loading new documents: 100%|█████████████████████| 1/1 [00:00<00:00, 120.48it/s]\n",
            "Loaded 1 new documents from privateGPT/source_documents\n",
            "Split into 91 chunks of text (max. 500 tokens each)\n",
            "Creating embeddings. May take some minutes...\n",
            "Using embedded DuckDB with persistence: data will be stored in: db\n",
            "Ingestion complete! You can now run privateGPT.py to query your documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python privateGPT/privateGPT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmk7ZmtsPhqA",
        "outputId": "cba62eac-747c-4673-d35c-2c6e8dfd8210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using embedded DuckDB with persistence: data will be stored in: db\n",
            "Found model file at  models/ggml-gpt4all-j-v1.3-groovy.bin\n",
            "gptj_model_load: loading model from 'models/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
            "gptj_model_load: n_vocab = 50400\n",
            "gptj_model_load: n_ctx   = 2048\n",
            "gptj_model_load: n_embd  = 4096\n",
            "gptj_model_load: n_head  = 16\n",
            "gptj_model_load: n_layer = 28\n",
            "gptj_model_load: n_rot   = 64\n",
            "gptj_model_load: f16     = 2\n",
            "gptj_model_load: ggml ctx size = 5401.45 MB\n",
            "gptj_model_load: kv self size  =  896.00 MB\n",
            "gptj_model_load: ................................... done\n",
            "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
            "\n",
            "Enter a query: What is the main topic of this speech?\n",
            "Exception ignored on calling ctypes callback function: <function LLModel._prompt_callback at 0x7a92a0a25cf0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpt4all/pyllmodel.py\", line 323, in _prompt_callback\n",
            "    @staticmethod\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    }
  ]
}